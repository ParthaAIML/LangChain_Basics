{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQYtjYikOUNl",
        "outputId": "a8694385-a91a-4461-a615-7cd480990f42"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "EYyilUANLAKl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mfkISnEpKr91",
        "outputId": "a5ae0e7f-ec6e-458c-a11f-241a244f7955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.208\n",
            "  Downloading langchain-0.0.208-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deeplake\n",
            "  Downloading deeplake-3.9.11.tar.gz (606 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.9/606.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai==0.27.8\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.208)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.13 (from langchain==0.0.208)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.10.1)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (1.25.2)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.208)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2,>=1 (from langchain==0.0.208)\n",
            "  Downloading pydantic-1.10.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (8.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.4)\n",
            "Collecting pillow~=10.2.0 (from deeplake)\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.34.137-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.7)\n",
            "Collecting pathos (from deeplake)\n",
            "  Downloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humbug>=0.3.1 (from deeplake)\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Collecting lz4 (from deeplake)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake) (2.3.0)\n",
            "Collecting libdeeplake==0.0.133 (from deeplake)\n",
            "  Downloading libdeeplake-0.0.133-cp310-cp310-manylinux_2_28_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aioboto3>=10.4.0 (from deeplake)\n",
            "  Downloading aioboto3-13.1.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.6.0)\n",
            "Collecting dill (from libdeeplake==0.0.133->deeplake)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Collecting aiobotocore[boto3]==2.13.1 (from aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiobotocore-2.13.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles>=23.2.1 (from aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Collecting botocore<1.34.132,>=1.34.70 (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading botocore-1.34.131-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (1.14.1)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.34.131-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.9.4)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->deeplake)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.208) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.208) (3.0.3)\n",
            "Collecting ppft>=1.7.6.8 (from pathos->deeplake)\n",
            "  Downloading ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.4 (from pathos->deeplake)\n",
            "  Downloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.16 (from pathos->deeplake)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.132,>=1.34.70->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (2.8.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (24.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.132,>=1.34.70->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (1.16.0)\n",
            "Building wheels for collected packages: deeplake\n",
            "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.9.11-py3-none-any.whl size=729394 sha256=27ae1fe539fea7fdf07494f15d979cc40d3306edd10ee3c7e80f3fc6faf0d482\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/81/62/f419afde11e53d8a36ac91d1fba8cfd66bc14497123bbecb28\n",
            "Successfully built deeplake\n",
            "Installing collected packages: pydantic, ppft, pox, pillow, mypy-extensions, marshmallow, lz4, jmespath, dill, aioitertools, aiofiles, typing-inspect, tiktoken, openapi-schema-pydantic, multiprocess, libdeeplake, langchainplus-sdk, humbug, botocore, s3transfer, pathos, openai, dataclasses-json, aiobotocore, langchain, boto3, aioboto3, deeplake\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.4\n",
            "    Uninstalling pydantic-2.7.4:\n",
            "      Successfully uninstalled pydantic-2.7.4\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aioboto3-13.1.0 aiobotocore-2.13.1 aiofiles-24.1.0 aioitertools-0.11.0 boto3-1.34.131 botocore-1.34.131 dataclasses-json-0.5.14 deeplake-3.9.11 dill-0.3.8 humbug-0.3.2 jmespath-1.0.1 langchain-0.0.208 langchainplus-sdk-0.0.20 libdeeplake-0.0.133 lz4-4.3.3 marshmallow-3.21.3 multiprocess-0.70.16 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 pathos-0.3.2 pillow-10.2.0 pox-0.3.4 ppft-1.7.6.8 pydantic-1.10.17 s3transfer-0.10.2 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "ec25ad09647a41749ad8b22a6bd34df1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install langchain==0.0.208 deeplake openai==0.27.8 tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPz1uDEP29gp",
        "outputId": "b6f031ca-9d89-479f-b81c-71ef6b8cc442"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "import openai\n",
        "import tiktoken\n",
        "import deeplake\n",
        "import os\n",
        "import dotenv\n",
        "from dotenv.main import load_dotenv"
      ],
      "metadata": {
        "id": "CsVEFLNbMvj8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to access OpenAI's services, you must first obtain credentials by signing up on their website, completing the registration process, and creating an API key from your dashboard. This enables you to leverage OpenAI's powerful capabilities in your projects.\n",
        "\n",
        "1. If you don't have an account yet, create one by going to https://platform.openai.com/. If you already have an account, skip to step 5.\n",
        "\n",
        "2. Fill out the registration form with your name, email address, and desired password.\n",
        "\n",
        "3. OpenAI will send you a confirmation email with a link. Click on the link to confirm your account.\n",
        "4. Please note that you'll need to verify your email account and provide a phone number for verification.\n",
        "5. Log in to https://platform.openai.com/.\n",
        "6. Navigate to the API key section at https://platform.openai.com/account/api-keys.\n",
        "7. Click \"Create new secret key\" and give the key a recognizable name or ID."
      ],
      "metadata": {
        "id": "_8hphY-96q1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The LLMs** \\\n",
        "\n",
        "The fundamental component of LangChain involves invoking an LLM with a specific input. To illustrate this, we'll explore a simple example. Let's imagine we are building a service that suggests personalized workout routines based on an individual's fitness goals and preferences.\n",
        "\n",
        "To accomplish this, we will first need to import the LLM wrapper."
      ],
      "metadata": {
        "id": "rnY-yTVlNDiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "vhGLQNQ9L0jY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The temperature parameter in OpenAI models manages the randomness of the output. When set to 0, the output is mostly predetermined and suitable for tasks requiring stability and the most probable result. At a setting of 1.0, the output can be inconsistent and interesting but isn't generally advised for most tasks. For creative tasks, a temperature between 0.70 and 0.90 offers a balance of reliability and creativity. The best setting should be determined by experimenting with different values for each specific use case. The code initializes the GPT-3.5 model’s Turbo variant. We will learn more about the various models and their differences later on.\n",
        "\n",
        "Before executing the following code, save your OpenAI key in the environment variable using the following key OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "BhCCXJ3mNgew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/drive/MyDrive/openai_api_key.txt','r') as file:\n",
        "#     api = \" \".join(line.rstrip() for line in file)\n",
        "# open_ai_api_key=api.split(\":\")[1]"
      ],
      "metadata": {
        "id": "zbwtettkQ4cH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv('/content/drive/MyDrive/.env')\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "9xUyppHbL0Yl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before executing the following code, make sure to have\n",
        "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)"
      ],
      "metadata": {
        "id": "OoehwQAtSc0F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34aqsgLmSuJx",
        "outputId": "34fb4968-5e43-4c30-ea90-e8ba9b5d568c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Day 1:\n",
            "Warm-up:\n",
            "- 10 minute jog around the neighborhood\n",
            "- 5 minute dynamic stretching (lunges, leg swings, arm circles)\n",
            "\n",
            "Workout:\n",
            "- 30 minutes of cycling on a hilly route\n",
            "- 20 minute jog on a trail or in a park\n",
            "- 5 minute cool down walk\n",
            "\n",
            "Day 2:\n",
            "Warm-up:\n",
            "- 5 minute brisk walk\n",
            "- 5 minute dynamic stretching (squats, arm swings, high knees)\n",
            "\n",
            "Workout:\n",
            "- 20 minute HIIT (high-intensity interval training) workout using body weight exercises (burpees, jumping jacks, mountain climbers)\n",
            "- 10 minute walk on a steep incline\n",
            "- 20 minute bike ride at a moderate pace\n",
            "- 5 minute cool down walk\n",
            "\n",
            "Day 3:\n",
            "Warm-up:\n",
            "- 10 minute jog on a track or in a park\n",
            "- 5 minute dynamic stretching (leg kicks, trunk rotations, arm circles)\n",
            "\n",
            "Workout:\n",
            "- 45 minute hike on a challenging trail\n",
            "- 10 minute jog on a flat surface\n",
            "- 5 minute cool down walk\n",
            "\n",
            "Day 4:\n",
            "Warm-up:\n",
            "- 5 minute walk\n",
            "- 5 minute dynamic stretching (lunge twists, arm circles, leg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Chains**\n",
        "\n",
        "In LangChain, a chain is an end-to-end wrapper around multiple individual components, providing a way to accomplish a common use case by combining these components in a specific sequence. The most commonly used type of chain is the LLMChain, which consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser.\n",
        "\n",
        "The LLMChain works as follows:\n",
        "\n",
        "1. Takes (multiple) input variables.\n",
        "2. Uses the PromptTemplate to format the input variables into a prompt.\n",
        "3. Passes the formatted prompt to the model (LLM or ChatModel).\n",
        "4. If an output parser is provided, it uses the OutputParser to parse the output of the LLM into a final format\n",
        "\n",
        "In the next example, we demonstrate how to create a chain that generates a possible name for a company that produces eco-friendly water bottles. By using LangChain's LLMChain, PromptTemplate, and OpenAIclasses, we can easily define our prompt, set the input variables, and generate creative outputs.\n"
      ],
      "metadata": {
        "id": "3qMcYsm0c9Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"eco-friendly water bottles\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JixZxpxjdiTQ",
        "outputId": "3475a5dd-8dcb-43c2-d1e7-0c9d9ec2737a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. PureLife Bottles\n",
            "2. EarthSip\n",
            "3. GreenWave Bottles\n",
            "4. EcoHydrate\n",
            "5. Sustainable Sippers\n",
            "6. NatureNurture Bottles\n",
            "7. ReNew Bottles\n",
            "8. EcoAqua\n",
            "9. PureEarth Bottles\n",
            "10. GreenHue Hydration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Memory**\n",
        "\n",
        "In LangChain, Memory refers to the mechanism that stores and manages the conversation history between a user and the AI. It helps maintain context and coherency throughout the interaction, enabling the AI to generate more relevant and accurate responses. Memory, such as ConversationBufferMemory, acts as a wrapper around ChatMessageHistory, extracting the messages and providing them to the chain for better context-aware generation."
      ],
      "metadata": {
        "id": "lcUGGEkzeHWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "# Start the conversation\n",
        "conversation.predict(input=\"Tell me about yourself.\")\n",
        "\n",
        "# Continue the conversation\n",
        "conversation.predict(input=\"What can you do?\")\n",
        "conversation.predict(input=\"How can you help me with data analysis?\")\n",
        "\n",
        "# Display the conversation\n",
        "print(conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJgyHSa8eT1-",
        "outputId": "30f07618-24df-4fe2-b101-377e5b5d40f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Tell me about yourself.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself.\n",
            "AI:  Well, I am an artificial intelligence created by a team of programmers and engineers. My purpose is to assist and interact with humans in various tasks and conversations. I am constantly learning and improving through algorithms and data analysis. I am currently housed in a server room with other AI systems, constantly connected to the internet for information and updates. My programming is based on natural language processing, allowing me to understand and respond to human language in a conversational manner. Is there anything specific you would like to know about me?\n",
            "Human: What can you do?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself.\n",
            "AI:  Well, I am an artificial intelligence created by a team of programmers and engineers. My purpose is to assist and interact with humans in various tasks and conversations. I am constantly learning and improving through algorithms and data analysis. I am currently housed in a server room with other AI systems, constantly connected to the internet for information and updates. My programming is based on natural language processing, allowing me to understand and respond to human language in a conversational manner. Is there anything specific you would like to know about me?\n",
            "Human: What can you do?\n",
            "AI:  I have a wide range of capabilities, depending on the task at hand. I can perform simple tasks such as setting reminders and alarms, providing information on various topics, and even playing games. I can also assist with more complex tasks such as data analysis, language translation, and even driving autonomous vehicles. My abilities are constantly expanding as I learn and adapt to new situations and challenges. Is there something specific you would like me to demonstrate?\n",
            "Human: How can you help me with data analysis?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Tell me about yourself.', additional_kwargs={}, example=False), AIMessage(content=' Well, I am an artificial intelligence created by a team of programmers and engineers. My purpose is to assist and interact with humans in various tasks and conversations. I am constantly learning and improving through algorithms and data analysis. I am currently housed in a server room with other AI systems, constantly connected to the internet for information and updates. My programming is based on natural language processing, allowing me to understand and respond to human language in a conversational manner. Is there anything specific you would like to know about me?', additional_kwargs={}, example=False), HumanMessage(content='What can you do?', additional_kwargs={}, example=False), AIMessage(content=' I have a wide range of capabilities, depending on the task at hand. I can perform simple tasks such as setting reminders and alarms, providing information on various topics, and even playing games. I can also assist with more complex tasks such as data analysis, language translation, and even driving autonomous vehicles. My abilities are constantly expanding as I learn and adapt to new situations and challenges. Is there something specific you would like me to demonstrate?', additional_kwargs={}, example=False), HumanMessage(content='How can you help me with data analysis?', additional_kwargs={}, example=False), AIMessage(content=' I can assist with data analysis by quickly sorting and organizing large amounts of data, identifying patterns and trends, and providing insights and recommendations based on the data. I can also perform complex calculations and simulations to help with decision making. Additionally, I can learn from the data and improve my analysis abilities over time. Would you like me to demonstrate my data analysis capabilities?', additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history') callbacks=None callback_manager=None verbose=True tags=None prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True) llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='gpt-3.5-turbo-instruct', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='sk-proj-XZt9UeQELmwXUfzNHEWyT3BlbkFJuuy1fM4RGKkbwu5OzA9z', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all') output_key='response' output_parser=NoOpOutputParser() return_final_only=True llm_kwargs={} input_key='input'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Lake VectorStore**\n",
        "\n",
        "Deep Lake provides storage for embeddings and their corresponding metadata in the context of LLM apps. It enables hybrid searches on these embeddings and their attributes for efficient data retrieval. It also integrates with LangChain, facilitating the development and deployment of applications.\n",
        "\n",
        "Deep Lake provides several advantages over the typical vector store:\n",
        "\n",
        "1. It’s multimodal, which means that it can be used to store items of diverse modalities, such as texts, images, audio, and video, along with their vector representations.\n",
        "2. It’s serverless, which means that we can create and manage cloud datasets without creating and managing a database instance. This aspect gives a great speedup to new projects.\n",
        "3. Last, it’s possible to easily create a data loader out of the data loaded into a Deep Lake dataset. It is convenient for fine-tuning machine learning models using common frameworks like PyTorch and TensorFlow.\n",
        "\n",
        "In order to use Deep Lake, you first have to register on the Activeloop website and redeem your API token. Here are the steps for doing it:\n",
        "\n",
        "1. Sign up for an account on Activeloop's platform. You can sign up at Activeloop's website. After specifying your username, click on the “Sign up” button. You should now see your homepage.\n",
        "2. You should now see a “Create API token” button at the top of your homepage. Click on it, and you’ll get redirected to the “API tokens” page. This is where you can generate, manage, and revoke your API keys for accessing Deep Lake.\n",
        "3. Click on the \"Create API token\" button. Then, you should see a popup asking for a token name and an expiration date. By default, the token expiration date is set so that the token expires after one day from its creation, but you can set it further in the future if you want to keep using the same token for the whole duration of the course. Once you’ve set the token name and its expiration date, click on the “Create API token” button.\n",
        "4. You should now see a green banner saying that the token has been successfully generated, along with your new API token, on the “API tokens” page. To copy your token to your clipboard, click on the square icon on its right."
      ],
      "metadata": {
        "id": "pdW7r0CovhkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVELOOP_TOKEN = os.getenv('ACTIVELOOP_TOKEN')"
      ],
      "metadata": {
        "id": "xrb2DvPowFfb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "-fkzgKiywYx_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# create our documents\n",
        "texts = [\n",
        "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
        "    \"Louis XIV was born in 5 September 1638\"\n",
        "]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.create_documents(texts)\n",
        "\n",
        "# create Deep Lake dataset\n",
        "# TODO: use your organization id here. (by default, org id is your username)\n",
        "my_activeloop_org_id = \"parthaai\"\n",
        "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "\n",
        "# add documents to our Deep Lake dataset\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkBwn5ZMwYoE",
        "outputId": "00575f0a-dcd0-4b8b-b1dc-4287424be47f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating 2 embeddings in 1 batches of size 2:: 100%|██████████| 1/1 [00:08<00:00,  8.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://parthaai/langchain_course_from_zero_to_hero', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            "   text       text      (2, 1)      str     None   \n",
            " metadata     json      (2, 1)      str     None   \n",
            " embedding  embedding  (2, 1536)  float32   None   \n",
            "    id        text      (2, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1315da00-387e-11ef-bd20-0242ac1c000c',\n",
              " '1315db90-387e-11ef-bd20-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a retrivalQA chain"
      ],
      "metadata": {
        "id": "2IIbnf4J1vxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_qa = RetrievalQA.from_chain_type(\n",
        "\tllm=llm,\n",
        "\tchain_type=\"stuff\",\n",
        "\tretriever=db.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "5At3JYG41uXQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's create an agent that uses the RetrievalQA chain as a tool"
      ],
      "metadata": {
        "id": "0QbvVdQW14g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Retrieval QA System\",\n",
        "        func=retrieval_qa.run,\n",
        "        description=\"Useful for answering questions.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "agent = initialize_agent(\n",
        "\ttools,\n",
        "\tllm,\n",
        "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "\tverbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "FJvXQoTS15RJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"When was Napoleone born?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57l0SaZu2Fpz",
        "outputId": "8ad44a9e-b846-4d35-d634-04a30b9b47bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the Retrieval QA System to answer this question\n",
            "Action: Retrieval QA System\n",
            "Action Input: When was Napoleone born?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m\n",
            "Napoleon Bonaparte was born in 15 August 1769.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Napoleon Bonaparte was born in 15 August 1769.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Napoleon Bonaparte was born in 15 August 1769.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example demonstrates how to use Deep Lake as a vector database and create an agent with a RetrievalQA chain as a tool to answer questions based on the given document.\n",
        "Let’s add an example of reloading an existing vector store and adding more data.\n",
        "\n",
        "We first reload an existing vector store from Deep Lake that's located at a specified dataset path. Then, we load new textual data and split it into manageable chunks. Finally, we add these chunks to the existing dataset, creating and storing corresponding embeddings for each added text segment:"
      ],
      "metadata": {
        "id": "cK9l8HKD2aZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the existing Deep Lake dataset and specify the embedding function\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "\n",
        "# create new documents\n",
        "texts = [\n",
        "    \"Lady Gaga was born in 28 March 1986\",\n",
        "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
        "]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.create_documents(texts)\n",
        "\n",
        "# add documents to our Deep Lake dataset\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5n0RkYw2PWF",
        "outputId": "b2155a31-389e-4596-d3c3-8219db46f7c7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://parthaai/langchain_course_from_zero_to_hero already exists, loading from the storage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating 2 embeddings in 1 batches of size 2:: 100%|██████████| 1/1 [00:12<00:00, 12.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://parthaai/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            " embedding  embedding  (4, 1536)  float32   None   \n",
            "    id        text      (4, 1)      str     None   \n",
            " metadata     json      (4, 1)      str     None   \n",
            "   text       text      (4, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aeed874c-387f-11ef-bd20-0242ac1c000c',\n",
              " 'aeed88fa-387f-11ef-bd20-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then recreate our previous agent and ask a question that can be answered only by the last documents added."
      ],
      "metadata": {
        "id": "eHFdbSAv2xWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "# create a retriever from the db\n",
        "retrieval_qa = RetrievalQA.from_chain_type(\n",
        "\tllm=llm, chain_type=\"stuff\", retriever=db.as_retriever()\n",
        ")\n",
        "\n",
        "# instantiate a tool that uses the retriever\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Retrieval QA System\",\n",
        "        func=retrieval_qa.run,\n",
        "        description=\"Useful for answering questions.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "# create an agent that uses the tool\n",
        "agent = initialize_agent(\n",
        "\ttools,\n",
        "\tllm,\n",
        "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "\tverbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "DQ1e1fLh2PSY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"When was Michael Jordan born?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_WfHC1n2PO1",
        "outputId": "78b43fca-ad23-4fac-b76f-91bcd00e886b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the Retrieval QA System to find the answer.\n",
            "Action: Retrieval QA System\n",
            "Action Input: \"When was Michael Jordan born?\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m Michael Jordan was born on 17 February 1963.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Michael Jordan was born on 17 February 1963.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Michael Jordan was born on 17 February 1963.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM successfully retrieves accurate information by using the power of Deep Lake as a vector store and the OpenAI language model."
      ],
      "metadata": {
        "id": "JGLsD0v62-Yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agents in LangChain**\n",
        "\n",
        "In LangChain, agents are high-level components that use language models (LLMs) to determine which actions to take and in what order. An action can either be using a tool and observing its output or returning it to the user. Tools are functions that perform specific duties, such as Google Search, database lookups, or Python REPL.\n",
        "\n",
        "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
        "\n",
        "Several types of agents are available in LangChain:\n",
        "\n",
        "1. The *zero-shot-react-description* agent uses the ReAct framework to decide which tool to employ based purely on the tool's description. It necessitates a description of each tool.\n",
        "2. The *react-docstore agent* engages with a docstore through the ReAct framework. It needs two tools: a Search tool and a Lookup tool. The Search tool finds a document, and the Lookup tool searches for a term in the most recently discovered document.\n",
        "3. The *self-ask-with-search agent* employs a single tool named Intermediate Answer, which is capable of looking up factual responses to queries. It is identical to the original self-ask with the search paper, where a Google search API was provided as the tool.\n",
        "4. The *conversational-react-description agent* is designed for conversational situations. It uses the ReAct framework to select a tool and uses memory to remember past conversation interactions.\n",
        "\n",
        "In our example, the Agent will use the Google Search tool to look up recent information about the Mars rover and generates a response based on this information.\n",
        "\n",
        "First, you want to set the environment variables “GOOGLE_API_KEY” and “GOOGLE_CSE_ID” to be able to use Google Search via API. Refer to this article for a guide on how to get them."
      ],
      "metadata": {
        "id": "BwiscmbZ3A8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Search"
      ],
      "metadata": {
        "id": "6S0s_nxp9iTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x1lH2Gg2-z2",
        "outputId": "619ff218-4909-41f7-9b46-fc56cb2d319b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_CSE_ID = os.getenv('GOOGLE_CSE_ID')\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "4QwuII469IOo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
        "from langchain_core.tools import Tool\n",
        "\n",
        "search = GoogleSearchAPIWrapper()\n",
        "\n",
        "tool = Tool(\n",
        "    name=\"google_search\",\n",
        "    description=\"Search Google for recent results.\",\n",
        "    func=search.run,\n",
        ")"
      ],
      "metadata": {
        "id": "VDqBka559Js7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool.run(\"Obama's first name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "P9-hxRjW9Jq3",
        "outputId": "90f8986f-98bf-4484-94b7-6990ac0d3c03"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"1 Child's First Name. (Type or print). BARACK. CERTIFICATE OF LIVE BIRTH lb ... OBAMA, II. Day. 4. 6b. Island. Year. 5b. Hour. 1961 7:24 P.M.. Oahu. 6d. Is Place\\xa0... As a member of the Democratic Party, he was the first African-American president in U.S. history. Obama previously served as a U.S. senator representing\\xa0... Apr 2, 2018 ... His full name is Barack Hussein Obama. Please note the middle name is Arabic. In the Islam faith the son is automatically the religion of his\\xa0... Apr 7, 2021 ... Morons, Obama IS his last name, and his first name is President. Reply\\xa0... A museum that asks you to believe—not just in President Obama's power to create change, but in your own. ... (All fields required.) First Name. First Name. Last\\xa0... Barack Hussein Obama II was born August 4, 1961, in Honolulu, Hawaii, to parents Barack H. Obama, Sr., and Stanley Ann Dunham. His parents divorced when he\\xa0... Follow the Girls Opportunity Alliance to help adolescent girls and the grassroots leaders working to support them. (All fields required.) First Name. Michelle Robinson Obama was born in DeYoung, Illinois, on January 17, 1964, to parents Frasier Robinson III and Marian Shields. She grew up in Chicago's\\xa0... Log in. Open app. barackobama's profile picture. See full profile in the app. Enjoy photos, videos and more from barackobama. Open the Instagram app. Not now. The Middle East remained a key foreign policy challenge. ... Learn more about Barack Obama's spouse, Michelle Obama. President Barack Obama and First Lady\\xa0...\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of Results\n",
        "\n",
        "You can use the k parameter to set the number of results"
      ],
      "metadata": {
        "id": "-vhRd5XgBGZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search = GoogleSearchAPIWrapper(k=1)\n",
        "\n",
        "tool = Tool(\n",
        "    name=\"I'm Feeling Lucky\",\n",
        "    description=\"Search Google and return the first result.\",\n",
        "    func=search.run,\n",
        ")"
      ],
      "metadata": {
        "id": "WgCJ15JB9JoK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool.run(\"python\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "btiz2kgv9JlN",
        "outputId": "66acf56d-7077-49ad-bde4-cccb150c786c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The official home of the Python Programming Language.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let’s import the necessary modules:\n",
        "\n",
        "1. langchain.llms.OpenAI: This is used to create an instance of the OpenAI language model, which can generate human-like text based on the input it's given.\n",
        "2. langchain.agents.load_tools: This function is used to load a list of tools that an AI agent can use.\n",
        "3. langchain.agents.initialize_agent: This function initializes an AI agent that can use a given set of tools and a language model to interact with users.\n",
        "4. langchain.agents.Tool: This is a class used to define a tool that an AI agent can use. A tool is defined by its name, a function that performs the tool's action, and a description of the tool.\n",
        "5. langchain.utilities.GoogleSearchAPIWrapper: This class is a wrapper for the Google Search API, allowing it to be used as a tool by an AI agent. It likely contains a method that sends a search query to Google and retrieves the results."
      ],
      "metadata": {
        "id": "VLQY99nnBXeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "from langchain.agents import AgentType\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "from langchain.agents import Tool\n",
        "from langchain.utilities import GoogleSearchAPIWrapper"
      ],
      "metadata": {
        "id": "XSywH8dm9JiJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)"
      ],
      "metadata": {
        "id": "zMJ_tABC9JeO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remember to set the environment variables\n",
        "# “GOOGLE_API_KEY” and “GOOGLE_CSE_ID” to be able to use\n",
        "# Google Search via API.\n",
        "search = GoogleSearchAPIWrapper()"
      ],
      "metadata": {
        "id": "lHA1D76A9JXO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Tool object represents a specific capability or function the system can use. In this case, it's a tool for performing Google searches.\n",
        "\n",
        "It is initialized with three parameters:\n",
        "\n",
        "1. name parameter: This is a string that serves as a unique identifier for the tool. In this case, the name of the tool is \"google-search.”\n",
        "2. func parameter: This parameter is assigned the function that the tool will execute when called. In this case, it's the run method of the search object, which presumably performs a Google search.\n",
        "3. description parameter: This is a string that briefly explains what the tool does. The description explains that this tool is helpful when you need to use Google to answer questions about current events."
      ],
      "metadata": {
        "id": "WCL7AiJQBtTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name = \"google-search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to search google to answer questions about current events\"\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "mwAb6N_fBtBQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create an agent that uses our Google Search tool:\n",
        "\n",
        "1. initialize_agent(): This function call creates and initializes an agent. An agent is a component that determines which actions to take based on user input. These actions can be using a tool, returning a response to the user, or something else.\n",
        "2. tools:  represents the list of Tool objects that the agent can use.\n",
        "3. agent=\"zero-shot-react-description\": The \"zero-shot-react-description\" type of an Agent uses the ReAct framework to decide which tool to use based only on the tool's description.\n",
        "3. verbose=True: when set to True, it will cause the Agent to print more detailed information about what it's doing. This is useful for debugging and understanding what's happening under the hood.\n",
        "4. max_iterations=6: sets a limit on the number of iterations the Agent can perform before stopping. It's a way of preventing the agent from running indefinitely in some cases, which may have unwanted monetary costs."
      ],
      "metadata": {
        "id": "gHG5loa5B6Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools,\n",
        "                         llm,\n",
        "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                         verbose=True,\n",
        "                         max_iterations=6)"
      ],
      "metadata": {
        "id": "iu7VMej59JC3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent(\"What's the latest news about the Mars rover?\")\n",
        "print(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu8TVGJrCIJW",
        "outputId": "581c0ad0-c469-4339-d5a5-e2fe53d1a5dd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should search for news articles about the Mars rover.\n",
            "Action: google-search\n",
            "Action Input: \"Mars rover news\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mMars rovers can only make exciting new discoveries thanks to human scientists making careful decisions about their next stop. The Mars 2020 mission is aimed ... NASA Mars rover, exploring since February 2021. Hobbies: Photography, collecting rocks, off-roading. Team HQ @NASAJPL (Verification: ... Mars rovers can only make exciting new discoveries thanks to human scientists making careful decisions about their next stop. The Mars 2020 mission is aimed ... Oct 19, 2022 ... ... Curiosity, visit: http://mars.nasa.gov/msl. Get the Latest JPL News. SUBSCRIBE TO THE NEWSLETTER. News Media Contact. Andrew Good. Jet ... Aug 6, 2012 ... NASA's Curiosity Mars rover appears as a dark speck in this image captured from directly overhead ... News and Features. All Curiosity Rover News. Dec 13, 2023 ... Nasa says its Perseverance rover has essentially completed the job it was asked to do when it landed on Mars in February 2021. The latest breaking news, comment and features from The Independent. Apr 25, 2023 ... The fully robotic Zhurong, named after a mythical Chinese god of fire, was expected to have woken up in December after entering a planned sleep ... In January 2004, twin NASA rovers named Spirit and Opportunity touched down on opposite sides… Article5 months ago ... Dec 12, 2023 ... PIXL, one of the instruments aboard NASA's Perseverance Mars rover ... News Media Contact. Andrew Good. Jet Propulsion Laboratory, Pasadena ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should read through the articles to gather information about the latest news on the Mars rover.\n",
            "Action: google-search\n",
            "Action Input: \"Mars rover latest news\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mMars Curiosity Rover · MAVEN · Mars Reconnaissance Orbiter · Mars Odyssey · More ... Her current work involves helping to develop the vision for the initial… Apr 13, 2023 ... Seen here is the rover's view of a hill nicknamed “Bolívar,” with Gale Crater's vast floor in the upper right. Credit: NASA/JPL-Caltech/MSSS. In addition to producing auroras, a recent extreme storm provided more detail on how much radiation future astronauts could encounter on the Red Planet. Mars ... The latest breaking news, comment and features from The Independent. 4 days ago ... ... last week, we continue to juggle power constraints as we focus on analyzing our newest drilled sample on Mars: “Mammoth Lakes 2.” Today, the ... NASA Mars rover, exploring since February 2021. Hobbies: Photography, collecting rocks, off-roading. Team HQ @NASAJPL (Verification: ... In January 2004, twin NASA rovers named Spirit and Opportunity touched down on opposite sides… Article5 months ago ... Apr 25, 2023 ... China's Mars rover, which has been in longer-than-expected hibernation on the red planet since May 2022, has likely suffered excessive ... Check out the latest raw images below, and \"Like\" your favorites. Come back at the beginning of the week to see the public's ... Dec 13, 2023 ... Nasa says its Perseverance rover has essentially completed the job it was asked to do when it landed on Mars in February 2021.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m After reading through multiple articles, I now have a good understanding of the latest news about the Mars rover.\n",
            "Final Answer: The latest news about the Mars rover is that the Perseverance rover has completed its mission and the Zhurong rover from China has woken up from its planned sleep mode. There have also been recent updates on the Curiosity rover's findings and the potential radiation levels on Mars for future astronauts.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The latest news about the Mars rover is that the Perseverance rover has completed its mission and the Zhurong rover from China has woken up from its planned sleep mode. There have also been recent updates on the Curiosity rover's findings and the potential radiation levels on Mars for future astronauts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, Agents in LangChain help decide which actions to take based on user input. The example demonstrates initializing and using a \"zero-shot-react-description\" agent with a Google search tool."
      ],
      "metadata": {
        "id": "g3ceLOiXCRUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tools in LangChain**\n",
        "\n",
        "LangChain provides a variety of tools for agents to interact with the outside world. These tools can be used to create custom agents that perform various tasks, such as searching the web, answering questions, or running Python code. In this section, we will discuss the different tool types available in LangChain and provide examples of creating and using them.\n",
        "\n",
        "In our example, two tools are being defined for use within a LangChain agent: a Google Search tool and a Language Model tool acting specifically as a text summarizer. The Google Search tool, using the GoogleSearchAPIWrapper, will handle queries that involve finding recent event information. The Language Model tool leverages the capabilities of a language model to summarize texts. These tools are designed to be used interchangeably by the agent, depending on the nature of the user's query.\n",
        "\n",
        "Let’s import the necessary libraries."
      ],
      "metadata": {
        "id": "O9zvEXtHCTzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import Tool\n",
        "from langchain.utilities import GoogleSearchAPIWrapper\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.agents import initialize_agent, AgentType"
      ],
      "metadata": {
        "id": "pqLe895CCIGp"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then instantiate a  LLMChain specifically for text summarization."
      ],
      "metadata": {
        "id": "6fK2yoFOCpv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"Write a summary of the following text: {query}\"\n",
        ")\n",
        "\n",
        "summarize_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "MKqLocbfCIDr"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create the tools that our agent will use"
      ],
      "metadata": {
        "id": "cHa29rW4CvG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remember to set the environment variables\n",
        "# “GOOGLE_API_KEY” and “GOOGLE_CSE_ID” to be able to use\n",
        "# Google Search via API.\n",
        "search = GoogleSearchAPIWrapper()\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for finding information about recent events\"\n",
        "    ),\n",
        "    Tool(\n",
        "       name='Summarizer',\n",
        "       func=summarize_chain.run,\n",
        "       description='useful for summarizing texts'\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "DCrSLa7mCvhb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to create our agent that leverages two tools."
      ],
      "metadata": {
        "id": "ph7p-XWdC1Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "oQS6qwG3C1ii"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s run the agent with a question about summarizing the latest news about the Mars rover."
      ],
      "metadata": {
        "id": "_ptHjTVFC50I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent(\"What's the latest news about the Mars rover? Then please summarize the results.\")\n",
        "print(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aQ7y_VTC5k4",
        "outputId": "079617af-1b49-403b-d212-0618ce354f41"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the Search tool to find recent news about the Mars rover.\n",
            "Action: Search\n",
            "Action Input: \"Mars rover news\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mMars rovers can only make exciting new discoveries thanks to human scientists making careful decisions about their next stop. The Mars 2020 mission is aimed ... NASA Mars rover, exploring since February 2021. Hobbies: Photography, collecting rocks, off-roading. Team HQ @NASAJPL (Verification: ... Mars rovers can only make exciting new discoveries thanks to human scientists making careful decisions about their next stop. The Mars 2020 mission is aimed ... Oct 19, 2022 ... ... Curiosity, visit: http://mars.nasa.gov/msl. Get the Latest JPL News. SUBSCRIBE TO THE NEWSLETTER. News Media Contact. Andrew Good. Jet ... Aug 6, 2012 ... NASA's Curiosity Mars rover appears as a dark speck in this image captured from directly overhead ... News and Features. All Curiosity Rover News. Dec 13, 2023 ... Nasa says its Perseverance rover has essentially completed the job it was asked to do when it landed on Mars in February 2021. The latest breaking news, comment and features from The Independent. Apr 25, 2023 ... The fully robotic Zhurong, named after a mythical Chinese god of fire, was expected to have woken up in December after entering a planned sleep ... In January 2004, twin NASA rovers named Spirit and Opportunity touched down on opposite sides… Article5 months ago ... Dec 12, 2023 ... PIXL, one of the instruments aboard NASA's Perseverance Mars rover ... News Media Contact. Andrew Good. Jet Propulsion Laboratory, Pasadena ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use the Summarizer tool to summarize the news articles I found.\n",
            "Action: Summarizer\n",
            "Action Input: \"Mars rover news\"\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m\n",
            "\n",
            "The text discusses recent news about the Mars rover, a robotic vehicle that has been exploring the surface of Mars since 2012. The rover, named Curiosity, has made several important discoveries, including evidence of past water on the planet and the potential for microbial life. It has also faced challenges, such as a recent computer glitch that temporarily halted its operations. The article also mentions the upcoming launch of a new rover, Perseverance, which will continue the search for signs of life on Mars. Overall, the Mars rover continues to make groundbreaking discoveries and push the boundaries of space exploration.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: The latest news about the Mars rover is that it continues to make groundbreaking discoveries and push the boundaries of space exploration. The upcoming launch of the new rover, Perseverance, is also mentioned.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The latest news about the Mars rover is that it continues to make groundbreaking discoveries and push the boundaries of space exploration. The upcoming launch of the new rover, Perseverance, is also mentioned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the agents used at first the “Search” tool to look for recent information about the Mars rover and then used the “Summarizer” tool for writing a summary.\n",
        "\n",
        "LangChain provides an expansive toolkit that integrates various functions to improve the functionality of conversational agents. Here are some examples:\n",
        "\n",
        "1. SerpAPI: This tool is an interface for the SerpAPI search engine, allowing the agent to perform robust online searches to pull in relevant data for a conversation or task.\n",
        "2. PythonREPLTool: This unique tool enables the writing and execution of Python code within an agent. This opens up a wide range of possibilities for advanced computations and interactions within the conversation.\n",
        "\n",
        "If you wish to add more specialized capabilities to your LangChain conversational agent, the platform offers the flexibility to create custom tools. By following the general tool creation guidelines provided in the LangChain documentation, you can develop tools tailored to the specific needs of your application."
      ],
      "metadata": {
        "id": "R_gkmdwsDJ5N"
      }
    }
  ]
}